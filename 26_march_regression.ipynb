{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2c39a4-e10a-4368-b2e9-eac8f6c110df",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e249df-368f-4e76-bc53-4225af2ac30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1-Simple Linear Regression:\n",
    "    Simple linear regression is a statistical technique used to model the relationship between two variables:\n",
    "    an independent variable (predictor variable) and a dependent variable (response variable)\n",
    "    \n",
    "The equation for simple linear regression is:\n",
    "        Y = β0 + β1*X + ε\n",
    "        \n",
    "e.g.:Suppose we want to analyze the relationship between the number of hours studied (X) and the exam score \n",
    "(Y) of a group of students. We collect data from 20 students and perform a simple linear regression analysis.\n",
    "\n",
    "The model can be represented as:\n",
    "Exam Score = β0 + β1 * Hours Studied + ε\n",
    "        \n",
    "2-Multiple Linear Regression:\n",
    "    Multiple linear regression is an extension of simple linear regression that involves more than one \n",
    "    independent variable. It is used to model the relationship between a dependent variable and multiple \n",
    "    independent variables. The goal is to find the best-fitting hyperplane that minimizes the sum of squared\n",
    "    differences between the observed and predicted values. \n",
    "    \n",
    "The equation for multiple linear regression is:\n",
    "    Y = β0 + β1X1 + β2X2 + ... + βn*Xn + ε\n",
    "\n",
    "e.g.: Let's consider a real estate scenario where we want to predict the price of a house based on its area \n",
    "    (X1), number of bedrooms (X2), and location index (X3). We collect data from 50 houses and perform a \n",
    "    multiple linear regression analysis. The model can be represented as:\n",
    "    House Price = β0 + β1 * Area + β2 * Bedrooms + β3 * Location Index + ε"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c1dc9-e850-4458-b6a0-5a332eb93266",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a98c05-8766-4e3c-b6e2-119d8b8d2e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Linear regression relies on several assumptions to ensure the validity and accuracy of the model. \n",
    "Here are the key assumptions of linear regression:\n",
    "\n",
    "1-Linearity: \n",
    "    The relationship between the independent variables and the dependent variable is assumed to be linear. \n",
    "    This means that the effect of each independent variable on the dependent variable is constant across all\n",
    "    values.\n",
    "\n",
    "2-Independence: \n",
    "    The observations in the dataset are assumed to be independent of each other.\n",
    "    There should be no correlation or relationship between the residuals (the differences between the observed\n",
    "    and predicted values) in the dataset.\n",
    "\n",
    "3-Homoscedasticity: \n",
    "    The residuals have constant variance (homoscedasticity) across all levels of the independent variables.\n",
    "    This assumption ensures that the spread of the residuals is consistent across the range of predicted \n",
    "    values.\n",
    "    \n",
    "To check whether these assumptions hold in a given dataset, you can perform various diagnostic tests and \n",
    "visualizations:\n",
    "    1-Residual analysis\n",
    "    2-Normality test\n",
    "    3-Variance inflation factor (VIF)\n",
    "    4-Correlation matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524b3aa-e788-45ca-9987-099a0ae5f047",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057ff81-aba6-4507-be31-2d3c9af970e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a linear regression model, the slope and intercept represent the relationship between the independent \n",
    "variable(s) and the dependent variable.\n",
    "\n",
    "- In a linear regression model, the slope and intercept represent the relationship between the independent\n",
    "    variable(s) and the dependent variable.\n",
    "\n",
    "- The slope (β1, β2, etc.) represents the change in the dependent variable associated with a one-unit change \n",
    "    in the corresponding independent variable, holding all other independent variables constant. The slope \n",
    "    indicates the rate of change or the impact of the independent variable(s) on the dependent variable.\n",
    "\n",
    "Real World Example:\n",
    "    \n",
    "    Let's consider a real-world scenario where we want to predict the salary of employees based on their \n",
    "    years of experience. We collect data from a company and perform a simple linear regression analysis.\n",
    "\n",
    "    The regression equation is:\n",
    "    Salary = β0 + β1 * Years of Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e2887-2086-4cf5-b1ee-b1d0e48eecf6",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935a862-96a2-43b3-97b2-62c0c2f2f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent is an optimization algorithm used in machine learning to minimize the error or cost function\n",
    "of a model by adjusting its parameters. It is a common approach for training models such as linear regression,\n",
    "logistic regression, and neural networks.\n",
    "\n",
    "Here's how gradient descent works:\n",
    "\n",
    "1-Initialize Parameters: Start by initializing the parameters of the model with some arbitrary values.\n",
    "\n",
    "2-Calculate the Cost: Compute the cost function, which measures the difference between the model's predictions\n",
    "and the actual values. The cost function quantifies the error of the model.\n",
    "\n",
    "3-Calculate Gradients: Compute the gradients of the cost function with respect to each parameter. The \n",
    "gradients indicate the direction and magnitude of the steepest ascent or descent in the parameter space.\n",
    "\n",
    "4-Update Parameters: Update the parameters by taking a small step in the opposite direction of the gradients.\n",
    "This step is determined by the learning rate, which controls the size of the parameter updates.\n",
    "\n",
    "5-Repeat: Iterate steps 2-4 until convergence or a predefined number of iterations. Convergence occurs when\n",
    "the parameters reach a point where further updates do not significantly reduce the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863c980-4509-4aef-a899-c56f662bb207",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b16df8-d440-488f-a1f8-1137337d2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple linear regression is an extension of simple linear regression that involves predicting a dependent\n",
    "variable based on multiple independent variables. It is used when there is more than one predictor variable\n",
    "that can potentially influence the outcome variable.\n",
    "\n",
    "In multiple linear regression, the relationship between the dependent variable (Y) and the independent \n",
    "variables (X1, X2, ..., Xn) is modeled as:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βn*Xn + ε\n",
    "\n",
    "where:\n",
    "\n",
    "Y is the dependent variable or the variable to be predicted.\n",
    "\n",
    "X1, X2, ..., Xn are the independent variables or predictors.\n",
    "\n",
    "β0, β1, β2, ..., βn are the regression coefficients or slopes that represent the impact of each independent \n",
    "variable on the dependent variable.\n",
    "\n",
    "ε is the error term or residual that accounts for the unexplained variability in the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1edad4-c4d1-441e-b231-91e04eb85f40",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b3240-edf9-44b1-b4bb-08ee316b77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity in multiple linear regression:\n",
    "    Multicollinearity refers to a high correlation or linear relationship between two or more independent \n",
    "    variables in a multiple linear regression model. It can cause issues in the regression analysis by \n",
    "    affecting the accuracy and interpretability of the regression coefficients.\n",
    "\n",
    "    \n",
    "To detect multicollinearity, there are several methods you can use:\n",
    "\n",
    "1-Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. \n",
    "    A high correlation coefficient (close to +1 or -1) suggests potential multicollinearity.\n",
    "\n",
    "2-Variance Inflation Factor (VIF): Calculate the VIF for each independent variable. VIF measures how much\n",
    "    the variance of the estimated regression coefficient is increased due to multicollinearity. VIF values\n",
    "    greater than 1 indicate multicollinearity, with higher values indicating stronger multicollinearity.\n",
    "\n",
    "3-Tolerance: Tolerance is the reciprocal of VIF (1/VIF). A tolerance value close to 1 suggests low \n",
    "    multicollinearity, while values close to 0 indicate high multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008a85f-145a-441e-ad20-ce49c7dec641",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9e25e-8b04-4458-b9f9-bf312664df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent \n",
    "variable(s) and the dependent variable is modeled as an nth-degree polynomial. It extends the concept of linear \n",
    "regression by including polynomial terms (exponents) of the independent variables.\n",
    "\n",
    "In linear regression, the relationship between the independent variable(s) and the dependent variable is\n",
    "assumed to be linear. The model assumes a straight-line relationship, and the coefficients represent the \n",
    "slope and intercept of the line. However, in many real-world scenarios, the relationship between variables\n",
    "may not be linear, and a straight line may not adequately capture the pattern.\n",
    "\n",
    "Polynomial regression allows for more flexibility in modeling nonlinear relationships. By introducing\n",
    "polynomial terms of the independent variables (e.g., squared terms, cubic terms, etc.), the regression model\n",
    "can capture curved or nonlinear patterns in the data.\n",
    "\n",
    "The polynomial regression model can be written as:\n",
    "\n",
    "y = β₀ + β₁x + β₂x² + β₃x³ + ... + βₙxⁿ + ε\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable,\n",
    "x is the independent variable,\n",
    "β₀, β₁, β₂, ..., βₙ are the coefficients of the polynomial terms,\n",
    "ε is the error term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fce48-8333-4a09-833b-3893ccf6b52e",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166990c8-e369-4c9d-986c-ac65f1394c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1-Capturing Nonlinear Relationships: Polynomial regression can capture nonlinear patterns in the data, \n",
    "allowing for more flexible modeling. It can fit curves and capture complex relationships that linear \n",
    "regression may not be able to handle effectively.\n",
    "\n",
    "2-Better Fit to the Data: By introducing polynomial terms, the model can closely fit the data points and \n",
    "reduce the residual error. This can result in a higher goodness of fit and better predictions within the \n",
    "observed range of data.\n",
    "\n",
    "Disadvantages of Polynomial Regression compared to Linear Regression:\n",
    "\n",
    "1-Overfitting: Polynomial regression can be prone to overfitting if the degree of the polynomial is too high\n",
    "or if there is insufficient data. Overfitting occurs when the model captures noise or random fluctuations in\n",
    "the data, leading to poor generalization to unseen data.\n",
    "\n",
    "2-Increased Complexity: Polynomial regression introduces more complexity compared to linear regression. As \n",
    "the degree of the polynomial increases, the model becomes more complex and harder to interpret. This \n",
    "complexity can make it challenging to understand the individual effects of predictors on the response variable.\n",
    "\n",
    "\n",
    "3-Extrapolation Limitations: Polynomial regression is primarily suitable for interpolating within the range \n",
    "of observed data. Extrapolating beyond the observed range can lead to unreliable predictions, as the model's\n",
    "behavior outside the range may not be well-defined or accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302d50f-e138-4125-ac4a-80c3c7a7c0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
